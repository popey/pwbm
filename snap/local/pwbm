#!/bin/bash
# pwbm - Personal WayBack Machine
# 
# Simple archiver for taking snapshots of web pages for later perusal
# Leverages monolith to do the snapshotting
#
# Usage:
#   pwbm (url)  - add url to list of urls to be snapshotted
#   pwbm        - snapshot all urls

URLSFILE="pwbmurls.txt"
ARCHIVEFILE="archive"
LOGFILE="log.txt"

# Check if we are in a snap
if [ -n "$SNAP" ]; then
	# We are in a snap
	DATADIR=$SNAP_USER_COMMON
else
	DATADIR="."
fi

URLS=$DATADIR/$URLSFILE
ARCHIVEDIR=$DATADIR/$ARCHIVEFILE
LOG=$DATADIR/$LOGFILE
OPT=$1

setup() {
	# Check if archive directory exists
	if [ ! -d "$ARCHIVEDIR" ]; then
		mkdir -p "$ARCHIVEDIR"
	fi
	# Check if list of URLs exists
	if [ ! -f "$URLS" ]; then
		touch "$URLS"
	fi
}

addurl() {
	# Check if it's already in the list
	if grep --silent "$OPT" "$URLS"; then
		echo -e "$(date) \nAlready in database: $OPT" | tee -a "$LOG"
		exit 2
	else
		echo "$OPT" >> "$URLS"
		echo -e "$(date) \nAdded URL: $OPT" | tee -a "$LOG"
		exit 0
	fi
}

show_help() {
	echo "Usage: pwbm [url]"
	echo ""
	echo "Provide a URL to add that URL to the list of URLs for fetching."
	echo "Run as \"pwbm\" to fetch all URLs currently in the list."
}

parse_input() {
	# Check if they added a new url
	if [ -n "$OPT" ]; then 
		# Show help if they asked for it
		if [ a"$OPT" == "a-h" -o a"$OPT" == "a--help" ]; then
			show_help
			exit 0
		fi
		# Check if it really is a valid url
		echo "$OPT" | grep --silent ^http 
		VALIDURL=$?
		if [ "$VALIDURL" == "0" ]; then
			addurl "$OPT"
		else
			echo -e "$(date) \nInvalid URL: $OPT" | tee -a "$LOG"
			exit 1
		fi
	fi
}

update(){
	# Check urls list has at least one url
	if grep --silent ^http "$URLS"; then
  	  # Loop through urls
	  echo -e "$(date) Start processing URLs" | tee -a "$LOG"
	  while read u; do
		echo -e "$(date) Processing: $u" | tee -a "$LOG"
		proto="$(echo "$u" | grep :// | sed -e's,^\(.*://\).*,\1,g')"
		url=$(echo "$u" | sed -e s,"$proto",,g )
		user="$(echo "$url" | grep @ | cut -d@ -f1)"
		hostport=$(echo "$url" | sed -e s,"$user"@,,g | cut -d/ -f1)
		#host="$(echo "$hostport" | sed -e 's,:.*,,g')"
		path="$(echo "$url" | grep / | cut -d/ -f2-)"
		escaped_path=$(echo "$path" | sed 's,/$,,;s,/,__,g')
		DATESTAMP="$(date --iso-8601=seconds)"
		mkdir -p "$ARCHIVEDIR"/"$hostport"/"$escaped_path"/
		FILENAME=$ARCHIVEDIR/$hostport/$escaped_path/$DATESTAMP-index.html
		if monolith -s "$u" > "$FILENAME"
		then
			echo -e "$(date) Success saving $u to $FILENAME" | tee -a "$LOG"
			if wkhtmltoimage  --custom-header Connection Keep-Alive,Upgrade --custom-header-propagation --debug-javascript --javascript-delay 8000 $ARCHIVEDIR/$hostport/$escaped_path/$DATESTAMP-index.html $ARCHIVEDIR/$hostport/$escaped_path/$DATESTAMP-index.png
			then
				echo -e "$(date) Success converting $ARCHIVEDIR/$hostport/$escaped_path/$DATESTAMP-index.html $ARCHIVEDIR/$hostport/$escaped_path/$DATESTAMP-index.png"
			else
				echo -e "$(date) Failed converting $ARCHIVEDIR/$hostport/$escaped_path/$DATESTAMP-index.html $ARCHIVEDIR/$hostport/$escaped_path/$DATESTAMP-index.png"
			fi
		else
			echo -e "$(date) Error saving $u to $FILENAME" | tee -a "$LOG"
		fi
	  done < $URLS
	  echo -e "$(date) Finished processing URLs" | tee -a "$LOG"
	fi
}

setup
parse_input
update